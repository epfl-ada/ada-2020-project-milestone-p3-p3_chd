{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create a dataset that contains only the relevant information for our analysis. We do this, because the original database takes up 30Gb, and we only need a small fraction of this. The resulting dataset is around 40Mb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../database.sqlite\"\n",
    "database = sqlite3.connect(PATH)\n",
    "df = pd.read_sql(\"SELECT author FROM May2015\", database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a random number of users from the list of all Reddit users in May 2015. This new set of users is representative of the population. We choose to select 35000 users, because in the original paper they obtain a similar number of Twitter users (around 34000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.DataFrame()\n",
    "authors['author'] = df['author'].unique()\n",
    "authors_sample = authors.sample(n=35000, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the columns created_utc, subreddit_id, score_hidden, subreddit, author, score, and parent_id of the comments that belong to the users in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT created_utc, subreddit_id, score_hidden, subreddit, author, score, parent_id FROM May2015 WHERE author in ' + str(tuple(authors_sample['author']))\n",
    "dataset = pd.read_sql(query, database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```comments.csv``` is the file that we use for all the data analysis tasks. It contains around comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20/80 Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
